### **Техническое задание на разработку гибридного парсера Avito**

> Важно: соблюдайте условия использования Avito и применяйте инструмент только для законных целей. Мобильное API не является публично задокументированным и может изменяться без уведомления.

**1. Название проекта:**
`Avito Hybrid Scraper`

**2. Цель проекта:**
Разработать приложение для автоматизированного сбора данных об объявлениях с сайта Avito.ru, использующее гибридный подход для аутентификации и сбора данных с целью повышения надежности и обхода систем защиты.

**3. Обзор системы:**
Приложение будет состоять из двух основных модулей и вспомогательных компонентов:
*   **Модуль аутентификации (`AuthManager`):** Использует `undetected-chromedriver` для эмуляции входа пользователя в аккаунт Avito через браузер. Его задача — получить актуальные сессионные `cookie` и сохранить их. Поддерживает валидацию и автообновление cookies.
*   **Модуль сбора данных (`Scraper`):** Использует библиотеку `requests` с полученными `cookie` для высокоскоростного взаимодействия с мобильным API Avito (`m.avito.ru`) с целью сбора информации об объявлениях.
*   **Хранилище (`Storage`):** Абстракция для записи данных в CSV/JSON (и опционально БД), поддержка ротации файлов.
*   **Ограничение частоты и повторов:** Компоненты RateLimiter и RetryPolicy для rps, backoff и обработки 429/5xx.
*   **Адаптеры парсинга:** Модули для разных версий API (например, v9, v15), возможность быстрого переключения.

**4. Функциональные требования:**

**4.1. Конфигурация:**
*   Приложение считывает параметры из внешнего файла (`config.ini`/`config.json`) и/или переменных окружения (переменные имеют приоритет).
*   Параметры (расширенный список):
    *   auth: login, password, headless=true/false, user_data_dir, cookies_path, cookies_ttl_hours
    *   input: urls_file (если не используется веб-управление запросами)
    *   searches: storage=sqlite|file, sqlite_path, default_filters, max_concurrency_per_search, schedule_cron/interval
    *   output: format=csv|json|jsonl, path, rotate_filesize_mb, compress=true/false, encoding
    *   http: user_agent, timeout_sec, max_retries, retry_backoff_base, retry_backoff_max
    *   rate_limits: rps_global, rps_per_host, random_delay_min/max, sleep_on_429_sec
    *   proxy: enabled=true/false, http_proxy, https_proxy, rotate_on_codes=[403,429]
    *   scraper: max_pages, max_items_per_url, concurrency, feature_flags (например, fetch_phone=true/false), source=urls_file|db
    *   logging: level, logfile=app.log, json=true/false, rotate=true/false

**4.2. Модуль аутентификации (`AuthManager`):**
*   Должен использовать `undetected-chromedriver` для запуска браузера.
*   При запуске должен проверять наличие и валидность сохраненных `cookie` из файла.
    *   **Валидация:** Проверка осуществляется путем тестового запроса к защищенной странице профиля Avito. Если запрос успешен (код 200), сессия считается активной, и аутентификация через браузер пропускается.
*   Если `cookie` невалидны или отсутствуют:
    1.  Запускается браузер.
    2.  Открывается страница входа Avito.
    3.  Автоматически вводятся логин и пароль из `config`.
    4.  Выполняется нажатие на кнопку входа.
    5.  **Обработка CAPTCHA:** На данном этапе система должна ожидать ручного ввода CAPTCHA пользователем. В лог выводится сообщение "Пожалуйста, решите CAPTCHA в открытом окне браузера и нажмите Enter в консоли".
    6.  После успешного входа модуль должен извлечь `cookie` из сессии браузера.
    7.  Сохранить `cookie` в файл (например, `cookies.json`) для последующих запусков.

**4.3. Модуль сбора данных (`Scraper`):**
*   Загружает `cookie` из файла, подготовленного `AuthManager`.
*   Источники поисковых запросов:
    *   из `urls_file` (файловый режим) или
    *   из хранилища запросов (БД SQLite, таблица `searches`) при включенном веб-управлении.
*   Для каждого активного запроса (search_id) или URL:
    1.  Выполняет запросы к мобильному API Avito (`m.avito.ru/api/9/items`) для получения списка объявлений.
    2.  Реализует обработку пагинации (переход по страницам результатов поиска), пока не будут собраны все объявления или не будет достигнут лимит (если он задан).
    3.  Для каждого объявления из списка извлекает базовую информацию (ID, заголовок, цена, город, ссылка).
    4.  Для получения подробной информации, включая номер телефона, выполняет последовательные запросы к соответствующим эндпоинтам API:
        *   `/api/15/items/{item_id}` — для получения детальной информации (описание, параметры).
        *   `/api/1/items/{item_id}/phone` — для получения номера телефона (требует валидных `cookie`).
*   Собранные данные по каждому объявлению должны агрегироваться в единую структуру.
*   Привязка к запросам: для каждого найденного item_id сохраняется связь с `search_id` (многие-ко-многим), прогресс пагинации/чекпоинт ведется отдельно по каждому `search_id`.

**4.4. Хранение данных:**
*   Приложение должно сохранять собранные данные в файл.
*   Поддерживаемые форматы:
    *   **CSV:** каждая строка — одно объявление, столбцы — поля данных.
    *   **JSON:** список объектов, где каждый объект представляет собой одно объявление.
*   Обязательные поля для сохранения:
    *   ID объявления
    *   URL
    *   Заголовок
    *   Цена
    *   Имя продавца
    *   Номер телефона
    *   Адрес/Город
    *   Дата публикации
    *   Описание (первые 300 символов)
    *   URL главного изображения

**4.6. Веб-приложение для отображения статистики:**
*   Цель: онлайн-визуализа��ия собранной статистики по объявлениям и прогрессу скрапинга; управление поисковыми запросами.
*   Архитектура: веб-сервер (FastAPI/Flask) + шаблоны (Jinja2) + статические ресурсы.
*   Управление поисковыми запросами:
    *   Форма добавления нового запроса: текст запроса (q), опциональные фильтры (категория, город/регион, цена, наличие телефона, дата публикации и т.п.).
    *   Список запросов: активные/архивные, статус, время последнего запуска, кол-во объявлений, ошибки.
    *   Операции: запуск/пауза/остановка скрапинга по запросу, редактирование, архивирование/удаление.
*   Основной функционал:
    *   Дашборд: KPI (кол-во объявлений, новых/обновленных, доля с телефонами, ошибки, RPS, среднее время ответа, 429/403 счётчики).
    *   Графики: динамика по времени (по часам/дням), распределения по категориям/городам/цене.
    *   Таблица последних объявлений с фильтрами/поиском, пагинацией.
    *   Статус скрапинга по URL: текущая страница, прогресс, ошибки, время последнего успеха.
    *   Экспорт фрагмента данных (CSV/JSONL) через UI.
*   API (REST):
    *   GET /api/stats/summary — агрегаты KPI (глобально и по search_id).
    *   GET /api/stats/timeseries?metric=items_per_minute&range=24h&search_id=... — временные ряды.
    *   GET /api/items?search_id=...&query=...&city=...&page=... — список объявлений по запросу.
    *   GET /api/scrape/status?search_id=... — статус процессов по запросу.
    *   POST /api/searches — создать запрос; GET /api/searches — список; PATCH /api/searches/{id} — изменить; POST /api/searches/{id}/run|pause|stop — управление.
*   Безопасность: опциональная базовая авторизация/токен; rate limit на API.
*   Источник данных: кэш/SQLite. Схема SQLite (минимум):
    *   searches(id, query, filters_json, status, created_at, updated_at)
    *   items(id PRIMARY KEY, url, title, price_amount, price_currency, city, address, published_at, main_image_url, seller_name, phone_e164, raw_json)
    *   search_items(search_id, item_id, first_seen_at, last_seen_at, UNIQUE(search_id, item_id))
    *   stats_timeseries(ts, metric, value, search_id)
    *   scrape_status(search_id, last_page, last_run_at, last_error)

**4.7. Телеграм-бот для доставки данных:**
*   Цель: оперативная доставка новых объявлений и агрегатов в Telegram-канал/чат; управление подписками на конкретные поисковые запросы.
*   Архитектура: бот (`python-telegram-bot`/`aiogram`), отдельный процесс/служба.
*   Функционал отправки:
    *   Авто-рассылка новых объявлений (настраиваемые фильтры: категория, город, цена, наличие телефона).
    *   Батчинг сообщений: пакетная отправка (batch_size) с интервалом, с учетом лимитов Telegram.
    *   Форматы сообщений: карточка объявления (заголовок, цена, город, ссылка, превью изображения), агрегаты за период.
    *   Алерты: уведомления при 401/403/429 всплесках, падении скрапинга, превышении ошибок.
*   Команды бота:
    *   /start — описание функционала и подписка.
    *   /add_query <текст> [фильтры] — создать запрос (опционально; может быть доступно только в веб).
    *   /queries — список активных запросов с краткой статистикой.
    *   /summary <search_id> — сводка за период по конкретному запросу (24h по умолчанию).
    *   /latest <search_id> [N] — последние N объявлений по запросу.
    *   /filters — установить/показать фильтры доставки.
    *   /status [search_id] — состояние скрапер-процессов.
*   Конфигурация: bot_token, chat_id, parse_mode (Markdown/HTML), batching, фильтры по умолчанию.
*   Безопасность: white-list пользователей/чатов, ограничение частоты команд.

**4.5. Логирование и обработка ошибок:**
*   Все ключевые действия должны логироваться в консоль и в файл (`app.log`). Уровни логирования: INFO, WARNING, ERROR.
*   Приложение должно корректно обрабатывать ошибки:
    *   **Ошибки сети:** Реализовать повторные попытки запроса (retry) с нарастающей задержкой.
    *   **HTTP ошибки:** При получении кода `403 Forbidden` или `401 Unauthorized`, сессия считается невалидной, и парсинг останавливается с сообщением о необходимости повторной аутентификации.
    *   **HTTP `429 Too Many Requests`:** Приложение должно "засыпать" на указанный в `config` промежуток времени перед повторной попыткой.

**5. Нефункциональные требования:**

*   **Надежность:** Приложение не должно прекращать работу из-за ошибки с одним объявлением; оно должно залогировать ошибку и продолжить работу со следующим.
*   **Производительность:** Запросы к API должны выполняться с настраиваемой случайной задержкой для имитации человеческого поведения.
*   **Модульность:** Код должен быть разделен на логические модули (`auth_manager.py`, `scraper.py`, `main.py`).

**6. Технический стек:**

*   **Язык программирования:** Python 3.9+
*   **Основные библиотеки:**
    *   `undetected-chromedriver`: для модуля аутентификации.
    *   `selenium`: для управления браузером.
    *   `requests`: для выполнения HTTP-запросов к API.
    *   `configparser`: для работы с `config.ini`.
    *   `logging`: для ведения логов.
    *   `json`, `csv`: для работы с форматами данных.

**7. План реализации (этапы):**

1.  **Этап 1: Настройка окружения.** Создание структуры проекта, настройка `requirements.txt`.
2.  **Этап 2: Реализация модуля аутентификации.** Разработка логики входа через браузер и сохранения `cookie`.
3.  **Этап 3: Реализация базового скрапера.** Написание логики получения списка объявлений с одной страницы по URL без получения телефона.
4.  **Этап 4: Расширение скрапера.** Добавление пагинации и сбора детальной информации по каждому объявлению, включая номер телефона.
5.  **Этап 5: Финализация.** Реализация сохранения в CSV/JSON, полноценное логирование и обработка ошибок.
6.  **Этап 6: Тестирование и отладка.** Комплексное тестирование на реальных данных.